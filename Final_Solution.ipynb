{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem :--->   Detection of Intent from  a given sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-loading and Preprocessing:-\n",
    "\n",
    "     1.I Used filtering techniques like removing stop words filtering words using regular expressions\n",
    "      But if I do like this I was drastically loosing my prediction accuracy its shown me that not only I am removing\n",
    "      unimportant words but also loosing information.\n",
    "      \n",
    "     2.I Used Information Extraction Techniques like a Count Vectorizer,Tfidf,WordEmbeddings.Character lever information \n",
    "         Extraction Techniques.\n",
    "         \n",
    "     3.I used POS Tagging techique for grouping similar information.\n",
    "     \n",
    "     4.WordEmbeddings(By using word2vec and glove vectors I extracted Feature Matrix .npy file)\n",
    "         For this I did another analysis in google collab for sentence embedding extraction.\n",
    "         I used that Embedding in RNNs.\n",
    "         Tried to build WordEmbeddings from FastText and ELMO MOdels sake of resource problem I didnot applied.\n",
    "     5.Using analyzer I extracted character level and word level information this are available with \n",
    "             Vectorizer(tfidf,count)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/harish/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing import text, sequence\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('enron_train.csv',names = ['intent', 'Messages'],error_bad_lines=False)\n",
    "test = pd.read_csv('enron_test.csv',names = ['intent', 'Messages'],error_bad_lines=False)\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = data['Messages']\n",
    "trainDF['label'] = data['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pandas.DataFrame()\n",
    "testDF['text'] = test['Messages']\n",
    "testDF['label'] = test['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "##train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "train_x=trainDF['text']\n",
    "valid_x=testDF['text']\n",
    "train_y=trainDF['label']\n",
    "valid_y=testDF['label']\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     1857\n",
       "Yes    1719\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     683\n",
       "Yes    309\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" load the pre-trained word-embedding vectors \\nembeddings_index = {}\\nfor i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\\n    values = line.split()\\n    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\\n\\n# create a tokenizer \\ntoken = text.Tokenizer()\\ntoken.fit_on_texts(trainDF['text'])\\nword_index = token.word_index\\n\\n# convert text to sequence of tokens and pad them to ensure equal length vectors \\ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\\n\\n# create token-embedding mapping\\nembedding_matrix = numpy.zeros((len(word_index) + 1, 300))\\nfor word, i in word_index.items():\\n    embedding_vector = embeddings_index.get(word)\\n    if embedding_vector is not None:\\n        embedding_matrix[i] = embedding_vector\\n        \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "##embedding_matrix.dump(\"/content/drive/My Drive/my_matrix.\")\n",
    "#numpy.save('/content/drive/My Drive/test', embedding_matrix)\n",
    "##embedding_matrix= numpy.load(\"/content/drive/My Drive/my_matrix.dat\")\n",
    "embedding_matrix=np.load('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5076, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning Modeling Techiniques like a Discriminative  and Generative modeling techniques I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier,feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net==False:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label,epochs=10)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y),predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.6975806451612904\n",
      "NB, WordLevel TF-IDF:  0.6743951612903226\n",
      "NB, N-Gram Vectors:  0.7348790322580645\n",
      "NB, CharLevel Vectors:  0.717741935483871\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy,pred = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print \"NB, Count Vectors: \", accuracy\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy,pred = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print \"NB, WordLevel TF-IDF: \", accuracy\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy,pred = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print \"NB, N-Gram Vectors: \", accuracy\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy,pred = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print \"NB, CharLevel Vectors: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.7348790322580645\n",
      "LR, WordLevel TF-IDF:  0.7832661290322581\n",
      "LR, N-Gram Vectors:  0.7389112903225806\n",
      "LR, CharLevel Vectors:  0.7449596774193549\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy,pred = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print \"LR, Count Vectors: \", accuracy\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy,pred = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print \"LR, WordLevel TF-IDF: \", accuracy\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy,pred = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print \"LR, N-Gram Vectors: \", accuracy\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy,pred = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print \"LR, CharLevel Vectors: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.688508064516129\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy,pred = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print \"SVM, N-Gram Vectors: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.7530241935483871\n",
      "RF, WordLevel TF-IDF:  0.7479838709677419\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy,pred = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print \"RF, Count Vectors: \", accuracy\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy,pred = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print \"RF, WordLevel TF-IDF: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.7943548387096774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.8024193548387096\n",
      "Xgb, CharLevel Vectors:  0.8084677419354839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy,pred = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print \"Xgb, Count Vectors: \", accuracy\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy,pred = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print \"Xgb, WordLevel TF-IDF: \", accuracy\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy,pred = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print \"Xgb, CharLevel Vectors: \", accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning Modeling Techniques:-\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.6698     \n",
      "Epoch 2/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.5097     \n",
      "Epoch 3/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.3549     \n",
      "Epoch 4/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.2706     \n",
      "Epoch 5/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.2203     \n",
      "Epoch 6/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.1866     \n",
      "Epoch 7/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.1640     \n",
      "Epoch 8/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.1499     \n",
      "Epoch 9/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.1381     \n",
      "Epoch 10/10\n",
      "3576/3576 [==============================] - 1s - loss: 0.1304     \n",
      "NN, Ngram Level TF IDF Vectors 0.688508064516129\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy,pred = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print \"NN, Ngram Level TF IDF Vectors\",  accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Step for Converting Text to Sequence format for Deep learning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "#token = text.Tokenizer()\n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1154: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.6206     \n",
      "Epoch 2/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.5258     \n",
      "Epoch 3/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.4665     \n",
      "Epoch 4/10\n",
      "3576/3576 [==============================] - 4s - loss: 0.4098     \n",
      "Epoch 5/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.3528     \n",
      "Epoch 6/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.2973     \n",
      "Epoch 7/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.2476     \n",
      "Epoch 8/10\n",
      "3576/3576 [==============================] - 4s - loss: 0.2186     \n",
      "Epoch 9/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.1763     \n",
      "Epoch 10/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.1504     \n",
      "CNN, Word Embeddings 0.688508064516129\n"
     ]
    }
   ],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(5076, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy,pred = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print \"CNN, Word Embeddings\",  accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "3576/3576 [==============================] - 17s - loss: 0.6445    \n",
      "Epoch 2/10\n",
      "3576/3576 [==============================] - 17s - loss: 0.5757    \n",
      "Epoch 3/10\n",
      "3576/3576 [==============================] - 16s - loss: 0.5483    \n",
      "Epoch 4/10\n",
      "3576/3576 [==============================] - 18s - loss: 0.5260    \n",
      "Epoch 5/10\n",
      "3576/3576 [==============================] - 18s - loss: 0.5111    \n",
      "Epoch 6/10\n",
      "3576/3576 [==============================] - 19s - loss: 0.4820    - ETA: 1s - loss\n",
      "Epoch 7/10\n",
      "3576/3576 [==============================] - 18s - loss: 0.4729    \n",
      "Epoch 8/10\n",
      "3576/3576 [==============================] - 16s - loss: 0.4532    \n",
      "Epoch 9/10\n",
      "3576/3576 [==============================] - 18s - loss: 0.4423    \n",
      "Epoch 10/10\n",
      "3576/3576 [==============================] - 18s - loss: 0.4328    \n",
      "RNN-LSTM, Word Embeddings 0.688508064516129\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm():\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(5076, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy,pred = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print \"RNN-LSTM, Word Embeddings\",  accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3576/3576 [==============================] - 14s - loss: 0.6623    \n",
      "Epoch 2/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.5806    \n",
      "Epoch 3/10\n",
      "3576/3576 [==============================] - 13s - loss: 0.5509    \n",
      "Epoch 4/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.5243    \n",
      "Epoch 5/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.5073    \n",
      "Epoch 6/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.4830    \n",
      "Epoch 7/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.4645    \n",
      "Epoch 8/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.4466    \n",
      "Epoch 9/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.4233    \n",
      "Epoch 10/10\n",
      "3576/3576 [==============================] - 15s - loss: 0.3954    \n",
      "RNN-GRU, Word Embeddings 0.688508064516129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_rnn_gru():\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy,pred = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print \"RNN-GRU, Word Embeddings\",  accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3576/3576 [==============================] - 28s - loss: 0.6659    \n",
      "Epoch 2/10\n",
      "3576/3576 [==============================] - 27s - loss: 0.5714    \n",
      "Epoch 3/10\n",
      "3576/3576 [==============================] - 28s - loss: 0.5446    \n",
      "Epoch 4/10\n",
      "3576/3576 [==============================] - 27s - loss: 0.5200    \n",
      "Epoch 5/10\n",
      "3576/3576 [==============================] - 28s - loss: 0.5040    \n",
      "Epoch 6/10\n",
      "3576/3576 [==============================] - 27s - loss: 0.4787    \n",
      "Epoch 7/10\n",
      "3576/3576 [==============================] - 28s - loss: 0.4639    \n",
      "Epoch 8/10\n",
      "3576/3576 [==============================] - 28s - loss: 0.4416    \n",
      "Epoch 9/10\n",
      "3576/3576 [==============================] - 29s - loss: 0.4179    \n",
      "Epoch 10/10\n",
      "3576/3576 [==============================] - 27s - loss: 0.4036    \n",
      "RNN-Bidirectional, Word Embeddings 0.688508064516129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_bidirectional_rnn():\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy,pred = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print \"RNN-Bidirectional, Word Embeddings\",  accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3576/3576 [==============================] - 4s - loss: 0.6158     \n",
      "Epoch 2/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.5266     \n",
      "Epoch 3/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.4658     \n",
      "Epoch 4/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.4064     \n",
      "Epoch 5/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.3596     - ETA: 0s - loss: 0\n",
      "Epoch 6/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.3017     \n",
      "Epoch 7/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.2529     \n",
      "Epoch 8/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.2095     \n",
      "Epoch 9/10\n",
      "3576/3576 [==============================] - 6s - loss: 0.1745     \n",
      "Epoch 10/10\n",
      "3576/3576 [==============================] - 5s - loss: 0.1572     \n",
      "CNN, Word Embeddings 0.688508064516129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_rcnn():\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy,pred = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print \"CNN, Word Embeddings\",  accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Results\n",
    "\n",
    "From Above Analysis I came to the conclusion that XGboost with Charecter level Vectors Model Give good results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, CharLevel Vectors:  0.8084677419354839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy,pred = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print \"Xgb, CharLevel Vectors: \", accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.88      0.86       683\n",
      "          1       0.71      0.65      0.68       309\n",
      "\n",
      "avg / total       0.80      0.81      0.81       992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(valid_y, pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:0.700464997731844\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "model = xgboost.XGBClassifier()\n",
    "scoring = 'roc_auc'\n",
    "results = cross_val_score(model,xtrain_tfidf_ngram_chars.tocsc(), train_y,cv=5, scoring=scoring)\n",
    "print(\"AUROC:\"+str(results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[602  81]\n",
      " [109 200]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "predicted = model.predict(xvalid_tfidf_ngram_chars.tocsc())\n",
    "matrix = confusion_matrix(valid_y, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From The Above results we can say Model Can Able to Differenced messages containt intent with 70% Confidence.\n",
    "\n",
    "F1 Score is Also Showing that How much Our model Confident to predict Based on Precision and Recall.\n",
    "F1 -Score 81%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Aditional Research Analysis You can see my other notebook(Research_roughwork.ipynb) or python file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
